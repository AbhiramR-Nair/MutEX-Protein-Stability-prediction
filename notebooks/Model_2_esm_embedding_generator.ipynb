{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","mount_file_id":"1EPAw1a8I8UWW09iedznPtQzX-GO9czjU","authorship_tag":"ABX9TyOhW2v/2D6VLS0EsAu6plrN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"8xkSQnf-7MHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Enhanced ESM2 Embedding Extraction for Protein ŒîŒîG Prediction\n","==============================================================\n","\n","This notebook implements signal-amplified embedding extraction from ESM2-650M\n","to address weak mutation signals in protein stability prediction.\n","\n","Key Features:\n","- Multi-layer aggregation (layers 30-33)\n","- Derived mutation features (Œî, |Œî|, cosine, L2)\n","- Z-normalization with train-only statistics\n","- Lazy HDF5 loading for memory efficiency\n","- Comprehensive validation and diagnostics\n","\n","Author: Enhanced pipeline for Stage 2 training\n","\"\"\""],"metadata":{"id":"YNKsWHYz44qf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# CELL 1: SETUP AND DEPENDENCIES\n","################################################################################\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA\n","from scipy.stats import pearsonr\n","from pathlib import Path\n","from tqdm.auto import tqdm\n","import h5py\n","from typing import Tuple, List, Dict, Optional\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from dataclasses import dataclass\n","import json\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set plotting style\n","sns.set_style(\"whitegrid\")\n","plt.rcParams['figure.figsize'] = (12, 6)\n","\n","# Check device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","if device.type == 'cuda':\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"],"metadata":{"id":"-Byv-sRf48RW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# CELL 2: INSTALL ESM AND CREATE DIRECTORIES\n","################################################################################\n","\n","# Install fair-esm\n","import subprocess\n","import sys\n","\n","try:\n","    import esm\n","    print(\"‚úì ESM already installed\")\n","except ImportError:\n","    print(\"Installing fair-esm...\")\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fair-esm==2.0.0\", \"-q\"])\n","    import esm\n","    print(\"‚úì ESM installed successfully\")\n","\n","# Create directory structure\n","output_dir = Path(\"/content/drive/MyDrive/Protein_prediction_model/abyssal_embeddings/stage_2\")\n","output_dir.mkdir(parents=True, exist_ok=True)\n","\n","for split in ['train', 'val', 'test']:\n","    (output_dir / split).mkdir(exist_ok=True)\n","\n","(output_dir / 'visualizations').mkdir(exist_ok=True)\n","print(f\"‚úì Directory structure created at {output_dir}\")\n"],"metadata":{"id":"mUHGV6rh4_Yo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# CELL 3: LOAD MUTATION DATASET\n","################################################################################\n","\n","def load_mutation_data(csv_path: str) -> pd.DataFrame:\n","    \"\"\"\n","    Load and prepare mutation dataset.\n","\n","    Expected columns:\n","    - wt_seq: Wild-type sequence\n","    - mut_seq: Mutant sequence\n","    - pos: Mutation position (0-indexed)\n","    - ddG: Experimental ŒîŒîG (kcal/mol)\n","    - wt_cluster: Cluster ID for splitting\n","    \"\"\"\n","    df = pd.read_csv(csv_path)\n","\n","    # Split into train/val/test by clusters (80/10/10)\n","    train_p = [23, 3, 186, 100, 119, 174, 'HHH', 146, 225, '81', '47', 123, 29, 142, 181, 223, '53', 155,\n","            213, '99', 113, 16, 224, 147, '9', 150, '56', '49', 196, 107, 226, 232, 169, 191, 163, 127,\n","            'EEHEE', 161, '4', 11, 179, 28, '87', '73', 112, 120, '50', 104, 128, 168, 231, '59', 109,\n","            38, '76', '96', 199, 192, '93', 185, 'EEHH', 210, 'EHEE', 30, '62', 173, 206, '48', 190,\n","            '45', '74', 205, 27, 158, '97', 208, 215, '72', 166, 152, '89', 101, 194, 139, 102, 145,\n","            '65', '58', '44', '92', 24, '40', 36, 'hall', 135, 209, 230, '71', 121, '5', 193, '64']\n","\n","    test_p=[170, 117, 202, 132, 218, '88', 156, '57', 114, 32, '95', 'HEEH', 184, 129, '67', '51',\n","            '55', 4, 227]\n","\n","    val_p = [34, 164, 20, '7', 15, 149, 105]\n","\n","    df['split'] = 'train'\n","    df.loc[df['wt_cluster'].isin(val_p), 'split'] = 'val'\n","    df.loc[df['wt_cluster'].isin(test_p), 'split'] = 'test'\n","\n","    return df\n","\n","# Load your dataset\n","csv_path = \"/content/drive/MyDrive/Protein_prediction_model/k50_cleaned.csv\"\n","mutation_df = load_mutation_data(csv_path)\n","\n","print(\"Dataset Statistics:\")\n","print(f\"Total samples: {len(mutation_df)}\")\n","print(f\"Train: {len(mutation_df[mutation_df['split']=='train'])}\")\n","print(f\"Val: {len(mutation_df[mutation_df['split']=='val'])}\")\n","print(f\"Test: {len(mutation_df[mutation_df['split']=='test'])}\")\n","print(f\"\\nŒîŒîG range: [{mutation_df['ddG'].min():.2f}, {mutation_df['ddG'].max():.2f}]\")\n","print(f\"ŒîŒîG mean¬±std: {mutation_df['ddG'].mean():.2f}¬±{mutation_df['ddG'].std():.2f}\")\n","print(f\"Sequence length range: {mutation_df['wt_seq'].str.len().min()}-{mutation_df['wt_seq'].str.len().max()}\")"],"metadata":{"id":"L_rEJ_J55ERa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# CELL 4: ENHANCED ESM2 EXTRACTOR CLASS\n","################################################################################\n","\n","class ESM2EmbeddingExtractor:\n","    \"\"\"\n","    Extract multi-layer, signal-amplified embeddings from ESM2-650M.\n","\n","    Enhancements:\n","    - Uses ESM2-650M (1280-dim) instead of 8M\n","    - Aggregates last N layers via mean pooling\n","    - Maintains frozen weights for efficiency\n","    - Supports batch processing with GPU optimization\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        model_name: str = \"esm2_t33_650M_UR50D\",\n","        device: torch.device = None,\n","        batch_size: int = 16,\n","        num_layers_to_average: int = 4\n","    ):\n","        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.batch_size = batch_size\n","        self.num_layers_to_average = num_layers_to_average\n","\n","        print(f\"Loading {model_name}...\")\n","        self.model, self.alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n","        self.batch_converter = self.alphabet.get_batch_converter()\n","\n","        # Move to device and freeze\n","        self.model = self.model.to(self.device)\n","        self.model.eval()\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","\n","        # Model specifications\n","        self.embedding_dim = self.model.embed_dim  # 1280\n","        self.num_layers = self.model.num_layers    # 33\n","\n","        # Validate and set layer extraction\n","        assert num_layers_to_average <= self.num_layers\n","        self.layers_to_extract = list(range(\n","            self.num_layers - num_layers_to_average + 1,\n","            self.num_layers + 1\n","        ))\n","\n","        print(f\"‚úì Model: ESM2-650M\")\n","        print(f\"‚úì Embedding dim: {self.embedding_dim}\")\n","        print(f\"‚úì Layers to average: {self.layers_to_extract}\")\n","        print(f\"‚úì Device: {self.device}\")\n","        print(\"=\" * 60)\n","\n","    def extract_position_embedding(\n","        self,\n","        sequence: str,\n","        position: int\n","    ) -> np.ndarray:\n","        \"\"\"Extract multi-layer averaged embedding at position.\"\"\"\n","        data = [(\"protein\", sequence)]\n","        _, _, batch_tokens = self.batch_converter(data)\n","        batch_tokens = batch_tokens.to(self.device)\n","\n","        with torch.no_grad():\n","            results = self.model(batch_tokens, repr_layers=self.layers_to_extract)\n","\n","        # Average across selected layers\n","        layer_embeddings = []\n","        for layer in self.layers_to_extract:\n","            emb = results[\"representations\"][layer]\n","            pos_emb = emb[0, position + 1, :]  # +1 for <cls> token\n","            layer_embeddings.append(pos_emb)\n","\n","        stacked = torch.stack(layer_embeddings, dim=0)\n","        averaged = torch.mean(stacked, dim=0)\n","\n","        return averaged.cpu().numpy()\n","\n","    def extract_batch_embeddings(\n","        self,\n","        sequences: List[str],\n","        positions: List[int]\n","    ) -> np.ndarray:\n","        \"\"\"Extract embeddings for a batch of sequences.\"\"\"\n","        data = [(f\"protein_{i}\", seq) for i, seq in enumerate(sequences)]\n","        _, _, batch_tokens = self.batch_converter(data)\n","        batch_tokens = batch_tokens.to(self.device)\n","\n","        with torch.no_grad():\n","            results = self.model(batch_tokens, repr_layers=self.layers_to_extract)\n","\n","        batch_embeddings = []\n","        for i, pos in enumerate(positions):\n","            layer_embs = []\n","            for layer in self.layers_to_extract:\n","                emb = results[\"representations\"][layer][i, pos + 1, :]\n","                layer_embs.append(emb)\n","\n","            stacked = torch.stack(layer_embs, dim=0)\n","            averaged = torch.mean(stacked, dim=0).cpu().numpy()\n","            batch_embeddings.append(averaged)\n","\n","        return np.array(batch_embeddings)\n","\n","# Initialize extractor\n","extractor = ESM2EmbeddingExtractor(\n","    device=device,\n","    batch_size=4,\n","    num_layers_to_average=4\n",")"],"metadata":{"id":"Ekx7UrWU5IKD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# CELL 5: DERIVED FEATURE COMPUTATION\n","################################################################################\n","\n","def compute_derived_features(\n","    wt_emb: np.ndarray,\n","    mut_emb: np.ndarray\n",") -> Dict[str, np.ndarray]:\n","    \"\"\"\n","    Compute mutation-specific derived features.\n","\n","    Features:\n","    - delta: mut - wt (directional change)\n","    - abs_delta: |mut - wt| (magnitude per dimension)\n","    - cosine_similarity: geometric alignment\n","    - l2_distance: Euclidean separation\n","    \"\"\"\n","    delta = mut_emb - wt_emb\n","    abs_delta = np.abs(delta)\n","\n","    wt_norm = np.linalg.norm(wt_emb)\n","    mut_norm = np.linalg.norm(mut_emb)\n","    cosine_sim = np.dot(wt_emb, mut_emb) / (wt_norm * mut_norm + 1e-8)\n","\n","    l2_dist = np.linalg.norm(delta)\n","\n","    return {\n","        'delta': delta,\n","        'abs_delta': abs_delta,\n","        'cosine_similarity': cosine_sim,\n","        'l2_distance': l2_dist\n","    }\n"],"metadata":{"id":"Q5uYkLLK5K96"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# CELL 6: EMBEDDING EXTRACTION AND CACHING\n","################################################################################\n","\n","def extract_and_cache_embeddings(\n","    mutation_df: pd.DataFrame,\n","    extractor: ESM2EmbeddingExtractor,\n","    output_dir: Path,\n","    batch_size: int = 32\n","):\n","    \"\"\"\n","    Extract embeddings with derived features and save to HDF5.\n","\n","    HDF5 structure per split:\n","    - wt_embeddings: (n_samples, 1280)\n","    - mut_embeddings: (n_samples, 1280)\n","    - delta_embeddings: (n_samples, 1280)\n","    - abs_delta_embeddings: (n_samples, 1280)\n","    - cosine_similarities: (n_samples,)\n","    - l2_distances: (n_samples,)\n","    - ddg_values: (n_samples,)\n","    - norm_stats/: normalization parameters (train only)\n","    \"\"\"\n","    train_stats = None\n","\n","    for split in ['train', 'val', 'test']:\n","        split_df = mutation_df[mutation_df['split'] == split].reset_index(drop=True)\n","        n_samples = len(split_df)\n","\n","        if n_samples == 0:\n","            continue\n","\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Processing {split.upper()} split: {n_samples} samples\")\n","        print(f\"{'='*60}\")\n","\n","        h5_path = output_dir / f\"{split}_embeddings.h5\"\n","\n","        with h5py.File(h5_path, 'w') as h5f:\n","            # Create datasets\n","            wt_ds = h5f.create_dataset('wt_embeddings', shape=(n_samples, extractor.embedding_dim), dtype='float32')\n","            mut_ds = h5f.create_dataset('mut_embeddings', shape=(n_samples, extractor.embedding_dim), dtype='float32')\n","            delta_ds = h5f.create_dataset('delta_embeddings', shape=(n_samples, extractor.embedding_dim), dtype='float32')\n","            abs_delta_ds = h5f.create_dataset('abs_delta_embeddings', shape=(n_samples, extractor.embedding_dim), dtype='float32')\n","            cosine_ds = h5f.create_dataset('cosine_similarities', shape=(n_samples,), dtype='float32')\n","            l2_ds = h5f.create_dataset('l2_distances', shape=(n_samples,), dtype='float32')\n","            ddg_ds = h5f.create_dataset('ddg_values', shape=(n_samples,), dtype='float32')\n","\n","            # Accumulators for train statistics\n","            if split == 'train':\n","                all_wt, all_mut, all_delta, all_abs_delta = [], [], [], []\n","\n","            # Process in batches\n","            for start_idx in tqdm(range(0, n_samples, batch_size), desc=f\"{split}\"):\n","                end_idx = min(start_idx + batch_size, n_samples)\n","                batch_df = split_df.iloc[start_idx:end_idx]\n","\n","                # Extract embeddings\n","                wt_seqs = batch_df['wt_seq'].tolist()\n","                mut_seqs = batch_df['mut_seq'].tolist()\n","                positions = batch_df['pos'].tolist()\n","\n","                wt_embs = extractor.extract_batch_embeddings(wt_seqs, positions)\n","                mut_embs = extractor.extract_batch_embeddings(mut_seqs, positions)\n","                ddg_vals = batch_df['ddG'].values\n","\n","                # Compute derived features\n","                batch_deltas, batch_abs_deltas = [], []\n","                batch_cosines, batch_l2s = [], []\n","\n","                for wt_emb, mut_emb in zip(wt_embs, mut_embs):\n","                    features = compute_derived_features(wt_emb, mut_emb)\n","                    batch_deltas.append(features['delta'])\n","                    batch_abs_deltas.append(features['abs_delta'])\n","                    batch_cosines.append(features['cosine_similarity'])\n","                    batch_l2s.append(features['l2_distance'])\n","\n","                batch_deltas = np.array(batch_deltas)\n","                batch_abs_deltas = np.array(batch_abs_deltas)\n","\n","                # Save to HDF5\n","                wt_ds[start_idx:end_idx] = wt_embs\n","                mut_ds[start_idx:end_idx] = mut_embs\n","                delta_ds[start_idx:end_idx] = batch_deltas\n","                abs_delta_ds[start_idx:end_idx] = batch_abs_deltas\n","                cosine_ds[start_idx:end_idx] = batch_cosines\n","                l2_ds[start_idx:end_idx] = batch_l2s\n","                ddg_ds[start_idx:end_idx] = ddg_vals\n","\n","                if split == 'train':\n","                    all_wt.append(wt_embs)\n","                    all_mut.append(mut_embs)\n","                    all_delta.append(batch_deltas)\n","                    all_abs_delta.append(batch_abs_deltas)\n","\n","                # GPU cleanup\n","                if device.type == 'cuda' and start_idx % (batch_size * 5) == 0:\n","                    torch.cuda.empty_cache()\n","\n","            # Compute train normalization statistics\n","            if split == 'train':\n","                all_wt = np.vstack(all_wt)\n","                all_mut = np.vstack(all_mut)\n","                all_delta = np.vstack(all_delta)\n","                all_abs_delta = np.vstack(all_abs_delta)\n","\n","                train_stats = {\n","                    'wt_mean': np.mean(all_wt, axis=0).astype(np.float32),\n","                    'wt_std': np.std(all_wt, axis=0).astype(np.float32) + 1e-8,\n","                    'mut_mean': np.mean(all_mut, axis=0).astype(np.float32),\n","                    'mut_std': np.std(all_mut, axis=0).astype(np.float32) + 1e-8,\n","                    'delta_mean': np.mean(all_delta, axis=0).astype(np.float32),\n","                    'delta_std': np.std(all_delta, axis=0).astype(np.float32) + 1e-8,\n","                    'abs_delta_mean': np.mean(all_abs_delta, axis=0).astype(np.float32),\n","                    'abs_delta_std': np.std(all_abs_delta, axis=0).astype(np.float32) + 1e-8,\n","                }\n","\n","                # Save to HDF5\n","                for key, value in train_stats.items():\n","                    h5f.create_dataset(f'norm_stats/{key}', data=value)\n","\n","                print(f\"\\n‚úì Normalization statistics computed from TRAIN\")\n","\n","        # Sanity checks\n","        with h5py.File(h5_path, 'r') as h5f:\n","            assert not np.isnan(h5f['wt_embeddings'][:]).any(), f\"{split}: WT has NaN\"\n","            assert not np.isnan(h5f['delta_embeddings'][:]).any(), f\"{split}: Œî has NaN\"\n","\n","            cosines = h5f['cosine_similarities'][:]\n","            assert (cosines >= -1.01).all() and (cosines <= 1.01).all()\n","\n","            delta_std = np.std(h5f['delta_embeddings'][:], axis=0)\n","            active_dims = (delta_std > 1e-6).sum()\n","            assert active_dims / len(delta_std) > 0.5, f\"{split}: Variance collapse\"\n","\n","            print(f\"‚úì {split} saved and validated ({n_samples} samples)\")\n","            print(f\"  Cosine: [{cosines.min():.3f}, {cosines.max():.3f}]\")\n","            print(f\"  L2: [{h5f['l2_distances'][:].min():.3f}, {h5f['l2_distances'][:].max():.3f}]\")\n","            print(f\"  Active dims: {active_dims}/{len(delta_std)}\")\n","\n","    # Save normalization stats globally\n","    if train_stats:\n","        np.savez(output_dir / 'normalization_stats.npz', **train_stats)\n","        print(f\"\\n‚úì Normalization stats saved\")\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"EMBEDDING EXTRACTION COMPLETE\")\n","    print(\"=\"*60)\n","\n","# Run extraction\n","extract_and_cache_embeddings(\n","    mutation_df=mutation_df,\n","    extractor=extractor,\n","    output_dir=output_dir,\n","    batch_size=64\n",")"],"metadata":{"id":"Uu7njjnp5QVe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# CELL 7: PYTORCH DATASET WITH LAZY LOADING\n","################################################################################\n","\n","@dataclass\n","class EmbeddingConfig:\n","    \"\"\"Configuration for feature loading.\"\"\"\n","    load_wt: bool = True\n","    load_mut: bool = True\n","    load_delta: bool = True\n","    load_abs_delta: bool = True\n","    load_cosine: bool = True\n","    load_l2: bool = True\n","    normalize: bool = True\n","\n","class MutationEmbeddingDataset(Dataset):\n","    \"\"\"\n","    PyTorch Dataset with signal-amplified embeddings and lazy loading.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        split: str,\n","        data_dir: Path,\n","        config: EmbeddingConfig = None,\n","        norm_stats: Optional[Dict] = None\n","    ):\n","        self.split = split\n","        self.h5_path = data_dir / f\"{split}_embeddings.h5\"\n","        self.config = config or EmbeddingConfig()\n","\n","        if not self.h5_path.exists():\n","            raise FileNotFoundError(f\"Not found: {self.h5_path}\")\n","\n","        with h5py.File(self.h5_path, 'r') as h5f:\n","            self.n_samples = h5f['wt_embeddings'].shape[0]\n","            self.embedding_dim = h5f['wt_embeddings'].shape[1]\n","\n","            if self.config.normalize:\n","                if norm_stats is not None:\n","                    self.norm_stats = norm_stats\n","                elif split == 'train' and 'norm_stats/wt_mean' in h5f:\n","                    # FIX: Correctly access datasets within the 'norm_stats' group\n","                    self.norm_stats = {\n","                        key: h5f['norm_stats'][key][:]\n","                        for key in h5f['norm_stats'].keys()\n","                    }\n","                else:\n","                    stats_path = data_dir / 'normalization_stats.npz'\n","                    if stats_path.exists():\n","                        loaded = np.load(stats_path)\n","                        self.norm_stats = {k: loaded[k] for k in loaded.files}\n","                    else:\n","                        print(f\"Warning: No norm stats, using raw embeddings\")\n","                        self.config.normalize = False\n","                        self.norm_stats = None\n","            else:\n","                self.norm_stats = None\n","\n","        if self.config.normalize and split != 'train':\n","            assert self.norm_stats is not None, f\"Need train stats for {split}\"\n","\n","        print(f\"‚úì {split}: {self.n_samples} samples, dim={self.embedding_dim}\")\n","        print(f\"  Normalization: {'ENABLED' if self.config.normalize else 'DISABLED'}\")\n","\n","    def __len__(self) -> int:\n","        return self.n_samples\n","\n","    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n","        sample = {}\n","\n","        with h5py.File(self.h5_path, 'r') as h5f:\n","            if self.config.load_wt:\n","                wt = h5f['wt_embeddings'][idx].astype(np.float32)\n","                if self.config.normalize and self.norm_stats:\n","                    wt = (wt - self.norm_stats['wt_mean']) / self.norm_stats['wt_std']\n","                sample['wt_embedding'] = torch.from_numpy(wt)\n","\n","            if self.config.load_mut:\n","                mut = h5f['mut_embeddings'][idx].astype(np.float32)\n","                if self.config.normalize and self.norm_stats:\n","                    mut = (mut - self.norm_stats['mut_mean']) / self.norm_stats['mut_std']\n","                sample['mut_embedding'] = torch.from_numpy(mut)\n","\n","            if self.config.load_delta:\n","                delta = h5f['delta_embeddings'][idx].astype(np.float32)\n","                if self.config.normalize and self.norm_stats:\n","                    delta = (delta - self.norm_stats['delta_mean']) / self.norm_stats['delta_std']\n","                sample['delta_embedding'] = torch.from_numpy(delta)\n","\n","            if self.config.load_abs_delta:\n","                abs_delta = h5f['abs_delta_embeddings'][idx].astype(np.float32)\n","                if self.config.normalize and self.norm_stats:\n","                    abs_delta = (abs_delta - self.norm_stats['abs_delta_mean']) / self.norm_stats['abs_delta_std']\n","                sample['abs_delta_embedding'] = torch.from_numpy(abs_delta)\n","\n","            if self.config.load_cosine:\n","                sample['cosine_similarity'] = torch.tensor(h5f['cosine_similarities'][idx], dtype=torch.float32)\n","\n","            if self.config.load_l2:\n","                sample['l2_distance'] = torch.tensor(h5f['l2_distances'][idx], dtype=torch.float32)\n","\n","            sample['ddg'] = torch.tensor(h5f['ddg_values'][idx], dtype=torch.float32)\n","\n","        return sample\n","\n","    def get_stats(self) -> Dict:\n","        with h5py.File(self.h5_path, 'r') as h5f:\n","            ddg = h5f['ddg_values'][:]\n","            cos = h5f['cosine_similarities'][:]\n","            l2 = h5f['l2_distances'][:]\n","\n","        return {\n","            'n_samples': self.n_samples,\n","            'ddg_mean': float(np.mean(ddg)),\n","            'ddg_std': float(np.std(ddg)),\n","            'ddg_range': [float(np.min(ddg)), float(np.max(ddg))],\n","            'cosine_mean': float(np.mean(cos)),\n","            'cosine_std': float(np.std(cos)),\n","            'l2_mean': float(np.mean(l2)),\n","            'l2_std': float(np.std(l2)),\n","        }\n","\n","# Create datasets\n","datasets = {}\n","config = EmbeddingConfig(normalize=True)\n","\n","if (output_dir / 'train_embeddings.h5').exists():\n","    datasets['train'] = MutationEmbeddingDataset('train', output_dir, config)\n","    train_norm_stats = datasets['train'].norm_stats\n","\n","    for split in ['val', 'test']:\n","        if (output_dir / f'{split}_embeddings.h5').exists():\n","            datasets[split] = MutationEmbeddingDataset(split, output_dir, config, train_norm_stats)\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"DATASETS CREATED\")\n","print(\"=\"*60)"],"metadata":{"id":"300iJ9kD5YIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# CELL 8: VALIDATION\n","################################################################################\n","\n","def validate_embeddings(datasets: Dict[str, MutationEmbeddingDataset]):\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"VALIDATION CHECKS\")\n","    print(\"=\"*60)\n","\n","    for split, dataset in datasets.items():\n","        print(f\"\\n{split.upper()}:\")\n","        print(\"-\" * 40)\n","\n","        stats = dataset.get_stats()\n","        print(f\"‚úì Samples: {stats['n_samples']}\")\n","        print(f\"‚úì ŒîŒîG: {stats['ddg_mean']:.2f}¬±{stats['ddg_std']:.2f}\")\n","        print(f\"‚úì Cosine: {stats['cosine_mean']:.3f}¬±{stats['cosine_std']:.3f}\")\n","        print(f\"‚úì L2: {stats['l2_mean']:.3f}¬±{stats['l2_std']:.3f}\")\n","\n","        # Sample checks\n","        for i in range(min(2, len(dataset))):\n","            sample = dataset[i]\n","\n","            for key, val in sample.items():\n","                if key != 'ddg':\n","                    assert not torch.isnan(val).any(), f\"{key} has NaN\"\n","                    assert not torch.isinf(val).any(), f\"{key} has Inf\"\n","\n","            print(f\"  Sample {i}: ŒîŒîG={sample['ddg']:.2f}, \" +\n","                  f\"L2={sample.get('l2_distance', 0):.3f}, \" +\n","                  f\"cos={sample.get('cosine_similarity', 0):.3f}\")\n","\n","        # DataLoader test\n","        loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0)\n","        batch = next(iter(loader))\n","        print(f\"‚úì DataLoader works, batch keys: {list(batch.keys())}\")\n","\n","validate_embeddings(datasets)"],"metadata":{"id":"I49Sv-em5cCh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# CELL 9: VISUALIZATION\n","################################################################################\n","\n","def visualize_embedding_quality(dataset, split, n_samples=200, save_path=None):\n","    print(f\"\\nVisualizing {split}...\")\n","    n_samples = min(n_samples, len(dataset))\n","\n","    wt_embs, mut_embs, delta_embs = [], [], []\n","    ddgs, cosines, l2s = [], [], []\n","\n","    for i in tqdm(range(n_samples), desc=\"Loading\"):\n","        s = dataset[i]\n","        wt_embs.append(s['wt_embedding'].numpy())\n","        mut_embs.append(s['mut_embedding'].numpy())\n","        delta_embs.append(s['delta_embedding'].numpy())\n","        ddgs.append(s['ddg'].item())\n","        cosines.append(s['cosine_similarity'].item())\n","        l2s.append(s['l2_distance'].item())\n","\n","    wt_embs = np.array(wt_embs)\n","    delta_embs = np.array(delta_embs)\n","    ddgs = np.array(ddgs)\n","    cosines = np.array(cosines)\n","    l2s = np.array(l2s)\n","\n","    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n","    fig.suptitle(f'Embedding Analysis - {split.upper()}', fontsize=16, fontweight='bold')\n","\n","    # 1. L2 distance\n","    axes[0, 0].hist(l2s, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n","    axes[0, 0].set_xlabel('L2 Distance')\n","    axes[0, 0].set_title('Mutation Distance')\n","    axes[0, 0].axvline(np.mean(l2s), color='red', linestyle='--', label=f'Mean: {np.mean(l2s):.2f}')\n","    axes[0, 0].legend()\n","\n","    # 2. L2 vs |ŒîŒîG|\n","    corr_l2, p_l2 = pearsonr(l2s, np.abs(ddgs))\n","    axes[0, 1].scatter(l2s, np.abs(ddgs), alpha=0.5, s=20)\n","    axes[0, 1].set_xlabel('L2 Distance')\n","    axes[0, 1].set_ylabel('|ŒîŒîG|')\n","    axes[0, 1].set_title('Distance vs Stability')\n","    axes[0, 1].text(0.05, 0.95, f'r={corr_l2:.3f}\\np={p_l2:.2e}',\n","                    transform=axes[0, 1].transAxes, va='top',\n","                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n","\n","    # 3. Cosine vs |ŒîŒîG|\n","    corr_cos, p_cos = pearsonr(cosines, np.abs(ddgs))\n","    axes[0, 2].scatter(cosines, np.abs(ddgs), alpha=0.5, s=20, color='coral')\n","    axes[0, 2].set_xlabel('Cosine Similarity')\n","    axes[0, 2].set_ylabel('|ŒîŒîG|')\n","    axes[0, 2].set_title('Similarity vs Stability')\n","    axes[0, 2].text(0.05, 0.95, f'r={corr_cos:.3f}\\np={p_cos:.2e}',\n","                    transform=axes[0, 2].transAxes, va='top',\n","                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n","\n","    # 4. Œî embedding norms\n","    delta_norms = np.linalg.norm(delta_embs, axis=1)\n","    axes[1, 0].hist(delta_norms, bins=30, edgecolor='black', alpha=0.7, color='mediumseagreen')\n","    axes[1, 0].set_xlabel('||Œî||')\n","    axes[1, 0].set_title('Œî Embedding Magnitude')\n","    axes[1, 0].axvline(np.mean(delta_norms), color='red', linestyle='--', label=f'Mean: {np.mean(delta_norms):.2f}')\n","    axes[1, 0].legend()\n","\n","    # 5. PCA of Œî\n","    pca = PCA(n_components=2)\n","    delta_2d = pca.fit_transform(delta_embs)\n","    scatter = axes[1, 1].scatter(delta_2d[:, 0], delta_2d[:, 1], c=ddgs,\n","                                  cmap='RdYlBu_r', alpha=0.6, s=30, edgecolors='black', linewidths=0.5)\n","    axes[1, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n","    axes[1, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n","    axes[1, 1].set_title('PCA of Œî Embeddings')\n","    plt.colorbar(scatter, ax=axes[1, 1], label='ŒîŒîG')\n","\n","    # 6. Cosine distribution\n","    axes[1, 2].hist(cosines, bins=30, edgecolor='black', alpha=0.7, color='mediumpurple')\n","    axes[1, 2].set_xlabel('Cosine Similarity')\n","    axes[1, 2].set_title('WT-Mutant Similarity')\n","    axes[1, 2].axvline(np.mean(cosines), color='red', linestyle='--', label=f'Mean: {np.mean(cosines):.3f}')\n","    axes[1, 2].legend()\n","\n","    plt.tight_layout()\n","    if save_path:\n","        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n","        print(f\"‚úì Saved to {save_path}\")\n","    plt.show()\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"STATISTICS - {split.upper()}\")\n","    print(f\"{'='*60}\")\n","    print(f\"L2 distance: {np.mean(l2s):.3f}¬±{np.std(l2s):.3f}\")\n","    print(f\"Cosine sim: {np.mean(cosines):.3f}¬±{np.std(cosines):.3f}\")\n","    print(f\"Corr (L2 vs |ŒîŒîG|): r={corr_l2:.3f}, p={p_l2:.2e}\")\n","    print(f\"Corr (cos vs |ŒîŒîG|): r={corr_cos:.3f}, p={p_cos:.2e}\")\n","    print(f\"Œî norm: {np.mean(delta_norms):.3f}¬±{np.std(delta_norms):.3f}\")\n","\n","# Run visualization\n","if 'train' in datasets:\n","    visualize_embedding_quality(\n","        datasets['train'],\n","        'train',\n","        n_samples=200,\n","        save_path=output_dir / 'visualizations' / 'train_analysis.png'\n","    )"],"metadata":{"id":"VhlKk-LK5hdm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# CELL 10: USAGE EXAMPLE\n","################################################################################\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"USAGE EXAMPLE\")\n","print(\"=\"*60)\n","\n","# Delta-only configuration (recommended)\n","delta_config = EmbeddingConfig(\n","    load_wt=False,\n","    load_mut=False,\n","    load_delta=True,\n","    load_abs_delta=True,\n","    load_cosine=True,\n","    load_l2=True,\n","    normalize=True\n",")\n","\n","train_loader = DataLoader(datasets['train'], batch_size=32, shuffle=True, num_workers=0)\n","batch = next(iter(train_loader))\n","\n","print(\"\\nBatch contents:\")\n","for key, val in batch.items():\n","    print(f\"  {key}: {val.shape if val.dim() > 1 else 'scalar'}\")\n","\n","# Example model\n","class SimpleDeltaPredictor(nn.Module):\n","    def __init__(self, embedding_dim=1280):\n","        super().__init__()\n","        input_dim = 2 * embedding_dim + 2  # delta + abs_delta + cos + L2\n","        self.network = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 1)\n","        )\n","\n","    def forward(self, batch):\n","        features = [\n","            batch['delta_embedding'],\n","            batch['abs_delta_embedding'],\n","            batch['cosine_similarity'].unsqueeze(1),\n","            batch['l2_distance'].unsqueeze(1)\n","        ]\n","        x = torch.cat(features, dim=1)\n","        return self.network(x)\n","\n","model = SimpleDeltaPredictor().to(device)\n","print(f\"\\n‚úì Model created: {sum(p.numel() for p in model.parameters()):,} parameters\")\n","print(f\"  Input dim: {2*1280 + 2}\")\n","\n","# Test forward pass\n","model.eval()\n","with torch.no_grad():\n","    batch_gpu = {k: v.to(device) for k, v in batch.items()}\n","    preds = model(batch_gpu)\n","    print(f\"\\n‚úì Forward pass: {preds.shape}\")\n","    print(f\"  Predictions: {preds[:3].squeeze().cpu().numpy()}\")\n","    print(f\"  True ŒîŒîG: {batch['ddg'][:3].numpy()}\")"],"metadata":{"id":"GLA52TT95kW4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# CELL 11: SIGNAL QUALITY ASSESSMENT\n","################################################################################\n","\n","def assess_signal_quality(dataset, n_samples=500):\n","    print(f\"\\nAssessing signal quality...\")\n","    n_samples = min(n_samples, len(dataset))\n","\n","    cosines, l2s, ddgs, delta_norms = [], [], [], []\n","\n","    for i in range(n_samples):\n","        s = dataset[i]\n","        cosines.append(s['cosine_similarity'].item())\n","        l2s.append(s['l2_distance'].item())\n","        ddgs.append(s['ddg'].item())\n","        delta_norms.append(torch.norm(s['delta_embedding']).item())\n","\n","    cosines = np.array(cosines)\n","    l2s = np.array(l2s)\n","    ddgs = np.array(ddgs)\n","    delta_norms = np.array(delta_norms)\n","\n","    corr_l2, p_l2 = pearsonr(l2s, np.abs(ddgs))\n","    corr_cos, p_cos = pearsonr(cosines, np.abs(ddgs))\n","\n","    print(f\"\\n{'='*60}\")\n","    print(\"SIGNAL QUALITY ASSESSMENT\")\n","    print(f\"{'='*60}\")\n","    print(f\"\\n1. WT-MUTANT COLLINEARITY:\")\n","    print(f\"   Cosine: {np.mean(cosines):.3f}¬±{np.std(cosines):.3f}\")\n","    print(f\"   Status: {'‚ö†Ô∏è  HIGH' if np.mean(cosines) > 0.95 else '‚úì Good'}\")\n","\n","    print(f\"\\n2. MUTATION SEPARATION:\")\n","    print(f\"   L2: {np.mean(l2s):.3f}¬±{np.std(l2s):.3f}\")\n","    print(f\"   Status: {'‚ö†Ô∏è  LOW' if np.mean(l2s) < 1.0 else '‚úì Good'}\")\n","\n","    print(f\"\\n3. Œî EMBEDDING VARIANCE:\")\n","    print(f\"   ||Œî||: {np.mean(delta_norms):.3f}¬±{np.std(delta_norms):.3f}\")\n","    print(f\"   Status: {'‚ö†Ô∏è  LOW' if np.mean(delta_norms) < 0.5 else '‚úì Good'}\")\n","\n","    print(f\"\\n4. CORRELATION WITH ŒîŒîG:\")\n","    print(f\"   L2 vs |ŒîŒîG|: r={corr_l2:.3f} (p={p_l2:.2e})\")\n","    print(f\"   Cos vs |ŒîŒîG|: r={corr_cos:.3f} (p={p_cos:.2e})\")\n","\n","    quality_score = 0\n","    if np.mean(cosines) < 0.95: quality_score += 1\n","    if np.mean(l2s) >= 1.0: quality_score += 1\n","    if abs(corr_l2) >= 0.2 and p_l2 < 0.01: quality_score += 1\n","    if np.mean(delta_norms) >= 0.5: quality_score += 1\n","\n","    print(f\"\\n5. OVERALL QUALITY: {quality_score}/4\")\n","    if quality_score >= 3:\n","        print(f\"   ‚úì GOOD - Ready for training\")\n","    elif quality_score == 2:\n","        print(f\"   ‚ö†Ô∏è  MODERATE - Consider additional features\")\n","    else:\n","        print(f\"   ‚ùå WEAK - May need fine-tuning\")\n","\n","    return quality_score\n","\n","if 'train' in datasets:\n","    assess_signal_quality(datasets['train'])"],"metadata":{"id":"ihm7ff6j5pGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# CELL 12: SAVE CONFIGURATION\n","################################################################################\n","\n","config_metadata = {\n","    'model': {\n","        'name': 'esm2_t33_650M_UR50D',\n","        'embedding_dim': 1280,\n","        'num_layers': 33,\n","        'layers_averaged': extractor.layers_to_extract\n","    },\n","    'features': {\n","        'wt_embeddings': 'Base WT (1280-dim)',\n","        'mut_embeddings': 'Base mutant (1280-dim)',\n","        'delta_embeddings': 'mut - wt (1280-dim)',\n","        'abs_delta_embeddings': '|mut - wt| (1280-dim)',\n","        'cosine_similarities': 'cos(wt, mut)',\n","        'l2_distances': '||mut - wt||'\n","    },\n","    'enhancements': {\n","        'multi_layer_averaging': True,\n","        'num_layers': len(extractor.layers_to_extract),\n","        'derived_features': True,\n","        'normalization': 'Z-score (train only)'\n","    },\n","    'splits': {split: datasets[split].get_stats() for split in datasets},\n","    'date': pd.Timestamp.now().isoformat()\n","}\n","\n","config_path = output_dir / 'embedding_config.json'\n","with open(config_path, 'w') as f:\n","    json.dump(config_metadata, f, indent=2)\n","\n","print(f\"\\n‚úì Configuration saved to {config_path}\")"],"metadata":{"id":"kdO7_Hhc5sBB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# SUMMARY\n","################################################################################\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"COMPLETE! üéâ\")\n","print(\"=\"*60)\n","print(\"\"\"\n","‚úì ESM2-650M embeddings extracted (1280-dim)\n","‚úì Multi-layer averaging (layers 30-33)\n","‚úì Derived features: Œî, |Œî|, cosine, L2\n","‚úì Z-normalization (train statistics only)\n","‚úì HDF5 lazy loading enabled\n","‚úì Datasets validated\n","‚úì Visualizations generated\n","\n","NEXT STEPS:\n","1. Use delta_config for training (most efficient)\n","2. Input: [Œî, |Œî|, cos, L2] = 2562 dims\n","3. Architecture: 2562 ‚Üí 512 ‚Üí 256 ‚Üí 128 ‚Üí 1\n","4. Monitor Pearson correlation on validation\n","5. Early stopping on best val correlation\n","\n","FILES CREATED:\n","\"\"\")\n","\n","for split in ['train', 'val', 'test']:\n","    h5_path = output_dir / f'{split}_embeddings.h5'\n","    if h5_path.exists():\n","        size = h5_path.stat().st_size / 1e6\n","        print(f\"  ‚Ä¢ {h5_path.name}: {size:.1f} MB\")\n","\n","print(f\"  ‚Ä¢ normalization_stats.npz\")\n","print(f\"  ‚Ä¢ embedding_config.json\")\n","print(f\"  ‚Ä¢ visualizations/train_analysis.png\")\n","\n","print(\"\\nREADY FOR STAGE 2 TRAINING! üöÄ\")"],"metadata":{"id":"ZJb_qeAy40_p"},"execution_count":null,"outputs":[]}]}